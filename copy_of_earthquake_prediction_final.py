# -*- coding: utf-8 -*-
"""Copy of Earthquake_Prediction_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cAOiLNeW0fTOQi10RNdRwfbFm83O_U1l

# Install and Import Dependencies:
"""

!pip install catboost

import pandas as pd
import numpy as np
import os
from catboost import CatBoostRegressor, Pool
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
import time
import warnings
warnings.filterwarnings('ignore')


import matplotlib.pyplot as plt

"""# Import Dataset from Kaggle:"""

from google.colab import files
uploaded = files.upload()
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c LANL-Earthquake-Prediction

!ls

"""## Extract Data:"""

!unzip LANL-Earthquake-Prediction.zip

"""# Data Analysis:"""

train_df = pd.read_csv('train.csv', nrows = 6000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})

train_df.head()

train_ad_sample = train_df['acoustic_data'].values[::100] # :: means skip by 100
train_ttf_sample = train_df['time_to_failure'].values[::100]

# twinx() - share same X-axis but with diff scales

fig, ax1 = plt.subplots(figsize=(12, 8))

plt.plot(train_ad_sample, color='r')
ax1.set_ylabel('acoustic data', color='r')
plt.legend(['acoustic data'], loc=(0.01, 0.95))

ax2 = ax1.twinx()

plt.plot(train_ttf_sample, color='b')
ax2.set_ylabel('time to failure', color='b')
plt.legend(['time to failure'], loc=(0.01, 0.9))

plt.grid(True)

"""# Feature Engineering:"""

def gen_features(X):
    strain = []
    #strain.append(X.mean())
    strain.append(X.std())
    strain.append(X.min())
    strain.append(X.max())
    #strain.append(X.kurtosis())
    #strain.append(X.skew())
    #strain.append(np.quantile(X,0.01))
    #strain.append(np.quantile(X,0.05))
    #strain.append(np.quantile(X,0.95))
    #strain.append(np.quantile(X,0.99))
    #strain.append(np.abs(X).min())
    strain.append(np.abs(X).max())
    #strain.append(np.abs(X).mean())
    strain.append(np.abs(X).std())
    return pd.Series(strain)

"""# Prepare Training Data:"""

train = pd.read_csv('train.csv', iterator = True, chunksize = 150000,
                    dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float64})  #Returns TextFileReader object for Iteration
X_train = pd.DataFrame()
y_train = pd.Series()


for df in train:
  ch = gen_features(df['acoustic_data'])
  X_train = X_train.append(ch, ignore_index=True)
  y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))

X_train.describe()

y_train.describe()

# scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train) # compute the mean and standard deviation to be used for later scaling
X_train_scaled = scaler.transform(X_train)

"""# Training Time:"""

from sklearn.ensemble import RandomForestRegressor
import time

rand_forest = RandomForestRegressor(n_estimators=100, oob_score=True, n_jobs=-1)

start_time = time.time()
rand_forest.fit(X_train_scaled, y_train.values.flatten())
training_time = time.time() - start_time

print("Training Time:", training_time)
print('Training Score:', rand_forest.score(X_train_scaled, y_train.values.flatten()))
print('OOB Score:', rand_forest.oob_score_)

rand_forest.feature_importances_

#svm

svr = SVR(kernel = 'rbf', tol = 0.01, C= 2, gamma=0.02)
start_time = time.time()
svr.fit(X_train_scaled, y_train.values.flatten())
print("Training Time:", time.time()-start_time)
print('Training Score:', svr.score(X_train_scaled, y_train.values.flatten()))

#catboost

pool = Pool(X_train, y_train)
cat_boost = CatBoostRegressor(loss_function='MAE', boosting_type='Ordered') # iterations=no. of trees
start_time = time.time()
cat_boost.fit(pool, silent=True) # don't show training | verbose
print("Time taken to train:", time.time()-start_time)
cat_boost.best_score_

"""## Using LSTM:"""

train_tfread = pd.read_csv('train.csv', iterator = True, chunksize = 150000, nrows = 6000000,
                       dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float32})
X_train_seq = []
y_train_seq = []
for df in train_tfread:
  X_train_seq.append(df['acoustic_data'].to_numpy().reshape(-1,1))
  y_train_seq.append(df['time_to_failure'].values[-1])

X_train_seq = np.array(X_train_seq)
y_train_seq = np.array(y_train_seq)
X_train_seq.shape[1:]

"""## Best Model:

Among all the models, `RandomForest` and `SVM` models were the best.<br>
Lets evaluate them on test data.<br>
Training LSTM network using sequence of 150000 also didn't work out. It was very time consuming to train and even the MAE was large.<br>
Although Rnadomforest and SVR are trained using best possible parameter values, you can fine-tune further. Or you can try other algorithms you know which can do better on this kind of dataset.

# Evaluate Models:

## Process test data:
"""

X_test = pd.DataFrame()
seg_ids = pd.Series()

for file in os.listdir('test'):
  data = pd.read_csv(os.path.join('test', file), dtype = {'acoustic_data': np.int16})
  data_fet = gen_features(data['acoustic_data'])
  X_test = X_test.append(data_fet, ignore_index=True)
  seg_ids = seg_ids.append(pd.Series(os.path.splitext(file)[0]))

X_test_scaled = scaler.transform(X_test)

"""## Make Predictions:"""

forest_pred = rand_forest.predict(X_test_scaled)
svr_pred = svr.predict(X_test_scaled)

svr_pred

forest_pred

"""### Generate Submission File:"""

sub = pd.DataFrame(columns=['seg_id', 'time_to_failure'])
sub['seg_id'] = seg_ids
sub['time_to_failure'] = forest_pred
sub.head()

sub.to_csv('submission_forest_latest_01.csv', index=False)

!ls

